# -*- coding: utf-8 -*-
"""ECE1513_A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11BhfsDMhzuhfRHLlUYOE-j059EqcCeFh

Name: Yiyu Wang  

URL: https://colab.research.google.com/drive/11BhfsDMhzuhfRHLlUYOE-j059EqcCeFh?usp=sharing
"""

# @title
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# @title
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.inspection import permutation_importance

# @title
from sklearn.preprocessing import StandardScaler, Normalizer
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Data preparation
data = pd.read_csv("indian_liver_patient.csv")
data.fillna(0,inplace=True)

variables =data.drop('Dataset', axis=1)
label = data['Dataset']-1 # shift to 0 and 1 for pytorch

# Male:1, Female:0
variables['Gender'] = variables['Gender'].map({'Male': 1, 'Female': 0})

X_train, X_test, y_train, y_test = train_test_split(variables, label, test_size= 0.3, random_state=42)

"""**Question a.** Use the original dataset without normalization to fine tune the hyper parameters such as number of layers and number of neurons per layer.Report the performance measures and plot the training and testing accuracy and f1-score across training epochs (two separate graphs).

**ANS.** First, Try 10 input features, 5 neurons in the first hidden layer and Output layer with 1 neuron for binary classification.
"""

# (10,5,1)
X_train_tensor = torch.FloatTensor(X_train.values)
X_test_tensor = torch.FloatTensor(X_test.values)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

#loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**NEXT STEP** Add more neurons at the first hidden layer. Try 10 input features, 10 neurons in the first hidden layer and Output layer with 1 neuron for binary classification."""

# @title
# (10,10,1)
X_train_tensor = torch.FloatTensor(X_train.values)
X_test_tensor = torch.FloatTensor(X_test.values)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 10)
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

#loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**NEXT STEP** Add one more layers. Try 10 input features, 10 neurons in the first hidden layer, 5 neurons in the second hidden layer, and Output layer with 1 neuron for binary classification."""

# @title
#(10,10,5,1)
X_train_tensor = torch.FloatTensor(X_train.values)
X_test_tensor = torch.FloatTensor(X_test.values)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.fc1 = nn.Linear(10, 10)
        self.fc2 = nn.Linear(10, 5)
        self.fc3 = nn.Linear(5, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

model = Net()

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():

        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)


plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**CONCLUSION**   
For the (10,5,1) model, The loss starts high but decreases over time, which is expected in training.
The training accuracy begins at around 40% and quickly improves, stabilizing around 70%.
The test accuracy also stabilizes at approximately 75%, the test accuracy is greater than the training accuracy.\
For the (10,10,1) model, Starts with a lower training and test accuracy but improves significantly over epochs. Training and test accuracy converge to around 72%. F1 score starts higher but fluctuates significantly, showing signs of potential instability in the model's performance on the dataset.\
The plots for the (10,10,5,1) model show that the model is very stable, but the lack of improvement over epochs and the low F1 score might indicate issues with the model's ability to handle the complexity of the data or that it is not learning any additional useful patterns beyond what is captured in the initial epochs.\
Overall, I pick (10,10,1) model for the following questions.

**Question b.** Train and test the model architecture developed in part (a) using the unbalanced data while using normalization and standardization.

**ANS.** From part(a), I decide to use model (10,10,1). First, Try use standardized data.
"""

# Standardize the data.
scaler = StandardScaler()
X_train_standardized = scaler.fit_transform(X_train)
X_test_standardized = scaler.transform(X_test)

# Normalize the data.
normalizer = Normalizer()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# @title
# standarized dataset

X_train_tensor = torch.FloatTensor(X_train_standardized)
X_test_tensor = torch.FloatTensor(X_test_standardized)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # You can adjust the sizes of these layers as needed
        self.fc1 = nn.Linear(10, 10)  # Assuming 10 input features
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

# loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**NEXT** Try use normalized data."""

# @title
# normalized dataset
X_train_tensor = torch.FloatTensor(X_train_normalized)
X_test_tensor = torch.FloatTensor(X_test_normalized)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # You can adjust the sizes of these layers as needed
        self.fc1 = nn.Linear(10, 10)  # Assuming 10 input features
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

# loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**CONCLUSION**   
Standardized Data: Produced a more stable accuracy and F1 score compared to the unbalanced data. The model also showed less overfitting as indicated by the closer training and test accuracies. The accurracy stabilized at the same level as the unbalanced data

Normalized Data: Demonstrated good generalization with very close training and test accuracies. However, the F1 score plateaued early and remained lower compared to the standardized data.

Based on the plots and the information for each dataset (unbalanced, standardized, and normalized), it seems that the standardized dataset has yielded the best results in terms of both accuracy and F1 score.

**Question c.** Retrain and test the model using different learning rates, specifically 0.1 and 0.000001.

**ANS.** From part(a) and part(b), I decide to use model (10,10,1) and standardized data. First, Try use learning rate 0.1.
"""

# @title
# learning rate 0.1
X_train_tensor = torch.FloatTensor(X_train_standardized)
X_test_tensor = torch.FloatTensor(X_test_standardized)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 10)
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

# loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.1)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**NEXT** learning rate 0.000001"""

# @title
# learning rate 0.000001
X_train_tensor = torch.FloatTensor(X_train_standardized)
X_test_tensor = torch.FloatTensor(X_test_standardized)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# data
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 10)
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

model = Net()

# loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.000001)

# Train
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # eval
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**CONCLUSION**  
For the learning rate of 0.1, the result show that it's too aggressive for this dataset and model architecture, causing the model weights to update in large steps, potentially jumping over minima in the loss landscape.

For the learning rate of 0.000001, the result show that the training process with such a low learning rate would be very slow, and you might not see much improvement in the early epochs. However, the model might show more gradual and stable improvements if given enough epochs to train.

Overall, comparing all three (0.001, 0.1, and 0.000001) learning rates, I still pick 0.001 as the learning rate for my model.

**Question d.** Knowing that the data is unbalanced, retrain and test the model after balancing the training set.

**ANS.** try oversampling techniques, using Synthetic Minority Over-sampling Technique (SMOTE) to oversample the minority class in the training set
"""

# oversampling SMOTE
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

scaler_res = StandardScaler()
X_train_res_scaled = scaler_res.fit_transform(X_train_res)
X_test_scaled = scaler_res.transform(X_test)

# @title
X_train_tensor = torch.FloatTensor(X_train_res_scaled)
X_test_tensor = torch.FloatTensor(X_test_scaled)
y_train_tensor = torch.FloatTensor(y_train_res.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# Create TensorDataset and DataLoader for train and test sets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Define the neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # You can adjust the sizes of these layers as needed
        self.fc1 = nn.Linear(10, 10)  # Assuming 10 input features
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

# Instantiate the model
model = Net()

# Define the loss function and optimizer
criterion = nn.BCELoss()  # Binary cross-entropy loss function
optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer

# Train the model
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Evaluate on training data
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**NEXT** Try undersampling method, perform undersampling using "RandomUnderSampler", which randomly removes samples from the majority class."""

# undersampling random
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)

scaler_un = StandardScaler()
X_train_un_scaled = scaler_un.fit_transform(X_train_un)
X_test_scaled = scaler_res.transform(X_test)

# @title
X_train_tensor = torch.FloatTensor(X_train_un.values)
X_test_tensor = torch.FloatTensor(X_test.values)
y_train_tensor = torch.FloatTensor(y_train_un.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# Create TensorDataset and DataLoader for train and test sets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Define the neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # You can adjust the sizes of these layers as needed
        self.fc1 = nn.Linear(10, 10)  # Assuming 10 input features
        self.fc2 = nn.Linear(10, 1)   # Output layer with one neuron for binary classification

    def forward(self, x):
        x = F.relu(self.fc1(x))   # ReLU activation for hidden layers
        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for output layer
        return x

# Instantiate the model
model = Net()

# Define the loss function and optimizer
criterion = nn.BCELoss()  # Binary cross-entropy loss function
optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer

# Train the model
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Evaluate on training data
    model.eval()
    with torch.no_grad():
        # Training performance
        train_preds = model(X_train_tensor).squeeze()
        train_preds = train_preds.round()
        train_acc = accuracy_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_accuracy.append(train_acc)
        train_f1_score = f1_score(y_train_tensor.numpy(), train_preds.detach().numpy())
        train_f1.append(train_f1_score)

        # Testing performance
        test_preds = model(X_test_tensor).squeeze()
        test_preds = test_preds.round()
        test_acc = accuracy_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_accuracy.append(test_acc)
        test_f1_score = f1_score(y_test_tensor.numpy(), test_preds.detach().numpy())
        test_f1.append(test_f1_score)

# Plotting training and testing accuracy
plt.plot(range(1, num_epochs+1), train_accuracy, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), test_accuracy, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and testing F1-Score
plt.plot(range(1, num_epochs+1), train_f1, label='Train F1 Score')
plt.plot(range(1, num_epochs+1), test_f1, label='Test F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

"""**CONCLUSION**  

**oversampling**: Comparing this to the outputs before balancing the dataset, there is a minor decrease in accuracy after learning process. but there is a big increase in F1 score, and keep increasing along the learning process.  
**Undersampling**: These results is not ideal and indicate that the undersampling technique used may have been too aggressive, removing too much information from the majority class.
overall, oversampling using SMOTE is a better choice

**Question e**. Use a scatter plot at the end to plot the data points and show the decision boundary of the best model you got on the same plot. Import the best model you got from assignment #1 and show its decision boundary too. Compare both models, and comment on the decision boundaries.

**ANS**. the best model I pick is (10,10,1) with learning rate 0.001 using SMOTE to balance the dataset and then standardize.The decision boundary is:
"""

# @title
from sklearn.decomposition import PCA

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(variables, label)
X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

X_train_tensor = torch.FloatTensor(X_train_pca)
y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))
X_test_tensor = torch.FloatTensor(X_test_pca)
y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(2, 10)  # Adjusted to 2 input features for PCA
        self.fc2 = nn.Linear(10, 5)
        self.fc3 = nn.Linear(5, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

model = Net()

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()


# Plotting function
def plot_decision_boundary(X, y, model, steps=1000, cmap='Paired'):

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x_span = np.linspace(x_min, x_max, steps)
    y_span = np.linspace(y_min, y_max, steps)
    xx, yy = np.meshgrid(x_span, y_span)

    with torch.no_grad():
        model.eval()
        predictions = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))
        Z = predictions.reshape(xx.shape).numpy()

    plt.contourf(xx, yy, Z, alpha=0.5, levels=np.linspace(0, 1, 3), cmap=cmap)
    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), alpha=0.5, cmap=cmap, edgecolors='k')
    plt.title('Neural network decision boundary')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()

plot_decision_boundary(X_train_pca, y_train, model)

"""**NEXT** The best model I pick from Assignment 1 is LDA classifier"""

# @title
lda_classifier = LinearDiscriminantAnalysis()
lda_model = lda_classifier.fit(X_train_pca, y_train)

x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))


Z = lda_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.5, levels=np.linspace(0, 1, 3),cmap='Paired')
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train,alpha=0.5,cmap='Paired',edgecolors='k')
plt.title('LDA Decision Boundary')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

"""**CONCLUSION**  
Based on the visual comparison of the decision boundaries, the model corresponding to the first image(Neutrel) appears to perform better for this particular dataset. The decision boundary is smooth and seems to capture the underlying distribution of the data points more effectively, suggesting a better fit to the complexity of the dataset. The points are well-separated by the boundary.

In contrast, the second image(lDA classifer) shows a linear decision boundary, which may be too simplistic for the data at hand. The linear approach does not seem to encapsulate the spread of the data points as well, potentially missing out on capturing more complex patterns within the data. Moreover, there are data points on either side of the boundary that do not align well with the predicted classes, suggesting that a linear model may not be sufficient for this case.
"""