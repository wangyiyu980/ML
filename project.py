# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NUBBSeLkUB4pOXzXdc9PitUX4nS3uTJF

#Machine Learning Approaches for Breast Cancer Classification: An Evaluation Using the Wisconsin Diagnostic Breast Cancer Dataset#

<p align="center">Yiyu Wang</p>

**1.Introduction**

Breast cancer remains a formidable health challenge globally, accounting for a substantial portion of cancer diagnoses among women.(Siegel, 2018)  Early detection and accurate classification of breast cancer can significantly enhance treatment outcomes and patient survival rates.(Sun, 2017) As such, the application of advanced data analytics and machine learning techniques in the medical field has gained momentum, with research demonstrating their potential to aid in diagnostic accuracy and clinical decision-making.(Kourou, 2015)

This study seeks to contribute to this field by evaluating the efficacy of various machine learning models to classify cases of breast cancer, using the well-regarded Wisconsin Diagnostic Breast Cancer (WDBC) dataset. With a focus on algorithms like Logistic Regression, Random Forest, Support Vector Machine (SVM), Decision Tree, and Linear Discriminant Analysis (LDA) and Neural Network (NN), this research provides a comparative analysis of these models.

**2.Methodology**  
**2.1 The Dataset**  
The machine learning algorithms were trained to detect breast
cancer using the Wisconsin Diagnostic Breast Cancer (WDBC)
dataset. The dataset consists of features which
were computed from a digitized image of a fine needle aspirate
(FNA) of a breast mass. The said features describe the
characteristics of the cell nuclei found in the image.There are 569 data points in the dataset: 212 Malignant, 357
Benign. Accordingly, the dataset features are as follows: (1)
radius,(2) texture, (3) perimeter, (4) area, (5) smoothness, (6)
compactness, (7) concavity, (8) concave points, (9) symmetry, and
(10) fractal dimension. With each feature having three
information: (i) mean, (ii) standard error, and (iii) “worst” or largest (mean of the three largest values) computed. Thus, having a
total of 30 dataset features.  
**2.2 Data preparation**  
For the purposes of model development and validation, we partitioned the dataset into a training set comprising 70% of the samples and a testing set constituting the remaining 30%. This division was executed employing the train_test_split() function, a component of the sklearn.model_selection module.

Prior to the application of machine learning algorithms, it is imperative to standardize the dataset. Models such as logistic regression, support vector machines (SVMs), and neural networks are particularly sensitive to variations in data scale. The process of standardization modifies the dataset such that each feature has a mean (μ) of zero and a standard deviation (σ) of one. The transformation is delineated by the formula:
$$z = \frac{X-μ}{σ}$$
,where
$X$ denotes the feature undergoing standardization, $μ$ signifies the mean, and
$σ$ represents the standard deviation of the feature. The practical implementation of this standardization was realized through the StandardScaler().fit_transform() method from the sklearn.preprocessing library.sklearn.preprocessing.  
**2.3 Machine Learning Algorithms**  
**2.3.1 Logistic Regression**     
Logistic Regression is a foundational algorithm in the field of machine learning, particularly esteemed for its effectiveness in binary classification tasks. (Bishop, 2006) This statistical method is adept at modeling the probability of a binary response based on one or more predictor variables. (Bishop, 2006) In the context of breast cancer detection, Logistic Regression is employed to ascertain the likelihood of a tumor being malignant or benign, based on a series of diagnostic measurements.

The underlying formula of Logistic Regression is predicated on the logistic function, defined as:
$$P(Y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \ldots + \beta_nX_n)}}$$

In this equation,

P(Y=1) epitomizes the probability of the tumor being malignant (class '1'),
e is the base of the natural logarithm, $β_0$ denotes the intercept, and
$β_1$...$β_n$ are the coefficients for each feature $X_1$...$X_n$ in the dataset
These coefficients are paramount as they quantify the relationship between the predictive features and the logarithm of the odds of the event that Y=1. The
Logistic Regression model was implemented using LogisticRegression() from sklearn.linear_model.  

**2.3.2 Random Forest**

Random Forest is an ensemble learning method for classification and regression that operates by constructing multiple decision trees at training time. The class output by the majority of the trees is taken as the model prediction. For classification problems, the decision of the majority of trees is considered as the final decision of the random forest model. The Random Forest model was implemented using RandomForestClassifier() from sklearn.ensemble.

**2.3.3 Support Vector Machine(SVM)**  
Support vector machine (SVM) is a powerful classifier that works by finding the hyperplane that best divides a dataset into two classes of data points.(Bishop, 2006) The hyperplane is chosen by maximizing the margin between the two classes. The SVM model can be represented as:
$$y=w^Tφ(X)+b$$

where $y$ is the output label, $w$ is the weight vector,
$φ(X)$ represents the transformed input data, and $b$ is the bias term. The SVM model was implemented using SVC() from sklearn.svm.  

**2.3.4 Decision Tree**  

A Decision Tree is a flowchart-like tree structure where an internal node represents a feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The root node is the starting point of the tree, and the deepest nodes are the decisions. The Decision Tree model was implemented using DecisionTreeClassifier() from sklearn.tree.

**2.3.5 Linear Discriminant Analysis (LDA)**  
LDA is a method used in statistics and machine learning to find the linear combination of features that best separates two or more classes of objects or events. It reduces dimensions while preserving as much of the class discriminatory information as possible. The LDA function is given by:

$$LDA(x)=w^Tx+b$$
where $x$ is an input vector, $w$ is the vector of coefficients, and $b$ is the bias. The LDA model was implemented using LinearDiscriminantAnalysis() from sklearn.discriminant_analysis.


**2.3.6 Neural Networks**  
In the realm of computational pattern recognition, neural networks stand as a robust algorithmic framework, drawing inspiration from the neural structure of intelligent organisms. For the task at hand, we employed a neural network architecture designed with a specific configuration to capture the complexity inherent to the diagnostic challenge of breast cancer. Our neural network was constructed with an input layer to accommodate the 30 features of the dataset, followed by two hidden layers with 15 and 8 neurons respectively, and culminating in an output layer with a single neuron to signify the binary classification—benign or malignant.

The architecture, implemented using PyTorch, is defined as follows: the first hidden layer transforms the input via 15 neurons, and the subsequent layer further refines this through 8 neurons, both employing the Rectified Linear Unit (ReLU) activation function to introduce non-linearity into the model. The output layer employs the sigmoid activation function, producing a probability distribution to facilitate binary classification. The training process involves backpropagation with Adam optimization and a Binary Cross-Entropy Loss (BCELoss) function over 100 epochs, ensuring iterative learning from the dataset.

**2.4 performance evaluation**  
The evaluation of model performance was conducted using a suite of key metrics: Accuracy, Precision, Recall, F1 Score, and ROC AUC Score. Accuracy reflects the proportion of total predictions that the model classified correctly, serving as a measure of its overall effectiveness. Precision assesses the accuracy of positive predictions—how many of the instances the model identified as positive were actually positive, thereby gauging the model's ability to minimize false positives. Recall measures the model's capacity to identify all actual positive cases, thus indicating its sensitivity to detecting relevant instances. The F1 Score, as the harmonic mean of Precision and Recall, offers a single metric that balances both sensitivity and precision, particularly valuable when dealing with imbalanced datasets. The ROC AUC Score evaluates the model's discrimination ability, with a higher score indicating a better distinction between classes across various decision thresholds. Models 2.3.1 to 2.3.5 utilized the functions accuracy_score(), precision_score(), f1_score(), recall_score(), and roc_auc_score() from sklearn.metrics for computing these metrics. For Model 2.3.6, the same metrics were calculated at each training epoch to monitor performance over time.

**3.Results:**   
This research was centered on the systematic evaluation of various machine learning models for their ability to classify breast cancer cases effectively, utilizing the Wisconsin Diagnostic Breast Cancer dataset. We conducted a thorough examination of several algorithms, which included Logistic Regression, Random Forest, Support Vector Machine (SVM), Decision Tree, and Linear Discriminant Analysis (LDA). These models underwent testing with a data partitioning of 70% for training and 30% for testing.

The performance metrics used to quantify the effectiveness of the models were: Accuracy, Precision, Recall, F1 Score, and ROC AUC Score. The findings are encapsulated in Table 1:

Logistic Regression demonstrated a high level of accuracy at 0.9825, coupled with a precision of 0.9667, recall of 0.9206, F1 Score of 0.9431, and an ROC AUC Score of 0.9511.
Random Forest achieved a similar precision to Logistic Regression but with a slightly lower accuracy of 0.9591, matching recall and F1 scores, and an identical ROC AUC Score of 0.9511.
SVM showcased robust performance with an accuracy of 0.9766, a precision of 0.9683, and excelled with a recall of 0.9683, yielding an F1 Score and an ROC AUC Score of 0.9683 and 0.9749, respectively.
The Decision Tree, with an accuracy of 0.9240 and precision of 0.8676, recorded a recall of 0.9365, an F1 Score of 0.9008, and an ROC AUC Score of 0.9266.
LDA offered solid performance with an accuracy of 0.9532, precision of 0.9661, recall of 0.9048, an F1 Score of 0.9344, and an ROC AUC Score of 0.9431.
In a novel approach, we also developed a neural network model using PyTorch, which was composed of a primary hidden layer with 15 neurons and a secondary one with 8 neurons, both employing the Rectified Linear Unit (ReLU) for activation. The output layer utilized the sigmoid function, crucial for the binary classification task at hand. Over the course of 100 epochs, as demonstrated in Figure 1, the model exhibited a significant increase in training accuracy, starting from a modest 38.19% and soaring to an impressive 99.24%. Correspondingly, Figure 2 highlights the model's consistent enhancement in the training F1 score, which underscores the model’s robust learning capabilities and its adeptness in adapting to the complexities inherent in the dataset.

<div align="center">
Table 1. Model Performance Metrics

| Model               | Accuracy | Precision | Recall | F1 Score | ROC AUC Score |
|---------------------|----------|-----------|--------|----------|---------------|
| Logistic Regression | 0.9825   | 0.9667    | 0.9206 | 0.9431   | 0.9511        |
| Random Forest       | 0.9591   | 0.9667    | 0.9206 | 0.9431   | 0.9511        |
| SVM                 | 0.9766   | 0.9683    | 0.9683 | 0.9683   | 0.9749        |
| Decision Tree       | 0.9240   | 0.8676    | 0.9365 | 0.9008   | 0.9266        |
| LDA                 | 0.9532   | 0.9661    | 0.9048 | 0.9344   | 0.9431        |

</div>
"""

# @title
plot_metrics({'train': train_f1, 'test': test_f1}, 'F1 Score per Epoch', 'F1 Score')

"""Figure 1: Training and Test Accuracy of the Neural Network Model Over 100 Epochs"""





# @title
plot_metrics({'train': train_f1, 'test': test_f1}, 'F1 Score per Epoch', 'F1 Score')

"""Figure 2: Training and Test F1 score of the Neural Network Model Over 100 Epochs

**4.Discussion:**  
In this study, we have rigorously evaluated a selection of machine learning models for their proficiency in classifying breast cancer through the Wisconsin Diagnostic Breast Cancer dataset. Each model showcased particular strengths which were assessed in detail.

The discussion is primarily focused on the relative effectiveness of these models. A standout performer was the neural network (NN) model, which exhibited remarkable results at the 100th epoch, achieving an accuracy of approximately 99.25%, precision of 100%, recall of about 98%, an F1 score close to 99%, and an ROC AUC score of roughly 98.99%. These metrics signal the NN model's superior capability in not only accurately classifying binary outcomes but also its adaptability to data it has not encountered before, as reflected by the high test scores.

In comparison to the other models, the NN model excelled, particularly with its impeccable precision, suggesting its high reliability in malignancy prediction. Its recall rate surpasses that of Logistic Regression model, Random Forest model, and LDA model, highlighting its vital role in identifying positive cases—a key aspect in medical diagnostics to avoid the perils of undiagnosed conditions.

The SVM model also demonstrated robustness across all metrics, with its recall matching that of the NN model, suggesting its substantial capability as a classifier. While Logistic Regression model and Random Forest model displayed commendable outcomes, they did not achieve the levels seen in the NN model or SVM model. The Decision Tree model, despite its interpretability, showed less precision, and LDA model, while stable, was slightly outperformed in recall.

The trend analysis from epoch progression indicates that the NN model's performance stabilizes early, with a negligible gap between training and test scores, implying effective learning and generalization.

In conclusion, all the models yielded promising results, yet the NN model's performance at the 100th epoch, reinforced by a consistent upward trend, positions it as a particularly powerful tool for breast cancer diagnosis using the WDBC dataset. Future research should extend these findings through external dataset validation and clinical trials to ensure the model's viability in practical scenarios.

However, the study is not without its limitations. The reliance on a single dataset may not reflect the entire scope of breast cancer diversity, potentially limiting the wider applicability of the results. Metrics like accuracy and F1 Score, while indicative, do not encompass all aspects of clinical relevance, such as the implications of false positives or negatives. The neural network's lack of transparency poses a challenge for clinical trust and integration, and the computational demands of model training and deployment were not considered, which could be a barrier in certain settings.

To overcome these limitations, future work should aim to validate the models on varied datasets, seek clinical insights to refine model development, improve the interpretability of complex models like neural networks, and consider the computational efficiency to facilitate broader implementation.

**5.Conclusions:**  

In summary, our study highlights the neural network model's exceptional ability to classify breast cancer, outperforming traditional machine learning models on the Wisconsin Diagnostic Breast Cancer dataset. Despite its complexity, the neural network demonstrated high accuracy and reliability, making it a promising tool for medical diagnostics. Future efforts will focus on enhancing the model's interpretability and validating its performance across broader datasets to ensure its applicability in diverse clinical settings.

**6.References:**  
Agarap, A. F. M. (2018, February). On breast cancer detection: an application of machine learning algorithms on the wisconsin diagnostic dataset. In Proceedings of the 2nd international conference on machine learning and soft computing (pp. 5-9).  
Bishop, C. M., & Nasrabadi, N. M. (2006). Pattern recognition and machine learning (Vol. 4, No. 4, p. 738). New York: springer.  
Kourou, K., Exarchos, T. P., Exarchos, K. P., Karamouzis, M. V., & Fotiadis, D. I. (2015). Machine learning applications in cancer prognosis and prediction. Computational and structural biotechnology journal, 13, 8-17.  

Siegel, R. L., Miller, K. D., & Jemal, A. (2018). Cancer statistics, 2018. CA: a cancer journal for clinicians, 68(1), 7-30.  
Sun, Y. S., Zhao, Z., Yang, Z. N., Xu, F., Lu, H. J., Zhu, Z. Y., ... & Zhu, H. P. (2017). Risk factors and preventions of breast cancer. International journal of biological sciences, 13(11), 1387.

**7.Appendices:**
"""

# library imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, Normalizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.inspection import permutation_importance
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# import data
data = pd.read_csv("data.csv")
# data preparation
data.fillna(0,inplace=True) # replacing missing value with 0
data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}) # M(malignant)：1 B(benign)：0
data = data.drop(columns=["id","Unnamed: 32"]) # drop useless column

X = data.drop('diagnosis', axis=1) # feature
Y = data[['diagnosis']] # label

# split the data  into 70% training and 30% testing samples
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size= 0.3, random_state=42)
# stadardized the data
scaler = StandardScaler()
x_train_standardized = scaler.fit_transform(x_train)
x_test_standardized = scaler.transform(x_test)

"""**Model Training**"""

# Logistic Regression
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(x_train_standardized, y_train.values.ravel())

# Prediction
log_reg_pred = log_reg.predict(x_test_standardized)

# Random Forest
rf = RandomForestClassifier()
rf.fit(x_train_standardized, y_train.values.ravel())

# Prediction
rf_pred = rf.predict(x_test_standardized)

# Support Vector Machine(SVM)
svm_model = SVC()
svm_model.fit(x_train_standardized, y_train.values.ravel())

# Predictions
svm_pred = svm_model.predict(x_test_standardized)

# Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(x_train_standardized, y_train.values.ravel())

# Predictions
dt_pred = dt_model.predict(x_test_standardized)

# Linear Discriminant Analysis (LDA)
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(x_train_standardized, y_train.values.ravel())

# Predictions
lda_pred = lda_model.predict(x_test_standardized)

# Evaluation Metrics
{
    "Logistic Regression": {
        "Accuracy": accuracy_score(y_test, log_reg_pred),
        "Precision": precision_score(y_test, rf_pred),
        "Recall": recall_score(y_test, rf_pred),
        "F1 Score": f1_score(y_test, rf_pred),
        "ROC AUC Score": roc_auc_score(y_test, rf_pred)
    },
    "Random Forest": {
        "Accuracy": accuracy_score(y_test, rf_pred),
        "Precision": precision_score(y_test, rf_pred),
        "Recall": recall_score(y_test, rf_pred),
        "F1 Score": f1_score(y_test, rf_pred),
        "ROC AUC Score": roc_auc_score(y_test, rf_pred)
    },
    "SVM" :{
    "Accuracy": accuracy_score(y_test, svm_pred),
    "Precision": precision_score(y_test, svm_pred),
    "Recall": recall_score(y_test, svm_pred),
    "F1 Score": f1_score(y_test, svm_pred),
    "ROC AUC Score": roc_auc_score(y_test, svm_pred)
    },
    "Decision Tree" : {
    "Accuracy": accuracy_score(y_test, dt_pred),
    "Precision": precision_score(y_test, dt_pred),
    "Recall": recall_score(y_test, dt_pred),
    "F1 Score": f1_score(y_test, dt_pred),
    "ROC AUC Score": roc_auc_score(y_test, dt_pred)
    },
    "LDA" : {
    "Accuracy": accuracy_score(y_test, lda_pred),
    "Precision": precision_score(y_test, lda_pred),
    "Recall": recall_score(y_test, lda_pred),
    "F1 Score": f1_score(y_test, lda_pred),
    "ROC AUC Score": roc_auc_score(y_test, lda_pred)
    }
}

# Neural Networks
# Convert the data to PyTorch tensors
X_train_tensor = torch.FloatTensor(x_train_standardized)
X_test_tensor = torch.FloatTensor(x_test_standardized)
y_train_tensor = torch.FloatTensor(y_train.values)
y_test_tensor = torch.FloatTensor(y_test.values)

# Create TensorDatasets and DataLoaders
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Define the neural network architecture
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(30, 15)  # First layer with 15 neurons
        self.fc2 = nn.Linear(15, 8)   # Second layer with 8 neurons
        self.fc3 = nn.Linear(8, 1)    # Output layer with 1 neuron

    def forward(self, x):
        x = F.relu(self.fc1(x))  # ReLU activation for first layer
        x = F.relu(self.fc2(x))  # ReLU activation for second layer
        x = torch.sigmoid(self.fc3(x))  # Sigmoid activation for output layer
        return x

model = Net()

# Loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 100
train_accuracy, test_accuracy = [], []
train_f1, test_f1 = [], []
train_precision, test_precision = [], []
train_recall, test_recall = [], []
train_roc_auc, test_roc_auc = [], []

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels.squeeze())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        # Training metrics
        train_preds = model(X_train_tensor).squeeze().round()
        train_accuracy.append(accuracy_score(y_train_tensor.numpy(), train_preds.numpy()))
        train_f1.append(f1_score(y_train_tensor.numpy(), train_preds.numpy()))
        train_precision.append(precision_score(y_train_tensor.numpy(), train_preds.numpy()))
        train_recall.append(recall_score(y_train_tensor.numpy(), train_preds.numpy()))
        train_roc_auc.append(roc_auc_score(y_train_tensor.numpy(), train_preds.numpy()))

        # Testing metrics
        test_preds = model(X_test_tensor).squeeze().round()
        test_accuracy.append(accuracy_score(y_test_tensor.numpy(), test_preds.numpy()))
        test_f1.append(f1_score(y_test_tensor.numpy(), test_preds.numpy()))
        test_precision.append(precision_score(y_test_tensor.numpy(), test_preds.numpy()))
        test_recall.append(recall_score(y_test_tensor.numpy(), test_preds.numpy()))
        test_roc_auc.append(roc_auc_score(y_test_tensor.numpy(), test_preds.numpy()))

# Plotting functions
def plot_metrics(metrics, title, ylabel):
    plt.plot(range(1, num_epochs+1), metrics['train'], label='Train')
    plt.plot(range(1, num_epochs+1), metrics['test'], label='Test')
    plt.xlabel('Epochs')
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend()
    plt.show()





#plot accuracy per epoch
plot_metrics({'train': train_accuracy, 'test': test_accuracy}, 'Accuracy per Epoch', 'Accuracy')

#plot F1 Score per epoch
plot_metrics({'train': train_f1, 'test': test_f1}, 'F1 Score per Epoch', 'F1 Score')

#plot Precision per epoch
plot_metrics({'train': train_precision, 'test': test_precision}, 'Precision per Epoch', 'Precision')

#plot Recall per epoch
plot_metrics({'train': train_recall, 'test': test_recall}, 'Recall per Epoch', 'Recall')

#plot ROC AUC Score per epoch
plot_metrics({'train': train_roc_auc, 'test': test_roc_auc}, 'ROC AUC Score per Epoch', 'ROC AUC Score')