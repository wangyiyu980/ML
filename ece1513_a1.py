# -*- coding: utf-8 -*-
"""ECE1513_A1 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12oUPQJKId4KM2x96_Q7KSpG4rVufSMrU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.inspection import permutation_importance

"""**Question a.**

Question a.
i. Supervised, unsupervised, and self-supervised learning.

Supervised learning is an approach where the model is trained on a labeled dataset, meaning for each input, the output is known. The goal is to learn a mapping from inputs to outputs and make predictions on new data.
Unsupervised learning involves training the model on an unlabeled dataset, aiming to find patterns or relationships in the data, such as clustering or dimensionality reduction.
Self-supervised Learning is A subset of supervised learning where labels are generated from the input data itself. The model is typically pre-trained on a task designed to create its own supervisory signal and then modified on a smaller, labeled dataset.

ii. Data splitting into training and testing:

The training set is used to train the model and the testing set is used to test the model's performance.

iii. Cross Validation:

A technique to assess a model's performance. The data is divided into 'k' subsets, and the model is trained 'k' times, each time using a different subset as the test set and the remaining data as the training set. The results are then averaged.

iv. Confusion Matrix:
A tabular representation of the actual vs. predicted classifications for a classification problem. It provides metrics like true positives, true negatives, false positives, and false negatives.

v. Model evaluation: define accuracy, sensitivity, and specificity:

Accuracy: The ratio of correctly predicted instances to the total instances.
Sensitivity: The ratio of correctly predicted positive observations to all actual positives.
Specificity: The ratio of correctly predicted negative observations to all actual negatives.

vi. Underfitting and Overfitting:

Underfitting: Occurs when the model is too simple to capture the underlying structure of the data, resulting in poor performance.
Overfitting: Occurs when the model becomes too complex and fits the noise in the data, performing well on the training set but lack generalization on new data.

vii. Balanced and unbalanced datasets:

A balanced dataset has roughly an equal number of samples for each class.
An unbalanced dataset has unequal distribution of samples for each class.

viii. Normalization vs Standardization:

Normalization: scaling features to lie between a given minimum and maximum value, for example between 0 and 1.

Standardization: shifting the distribution of each feature to have a mean of zero and a standard deviation of one.

ix. Neural Networks:

Neural networks consist of layers: an input layer, one or multiple hidden layers, and an output layer. Each node in a layer is connected to nodes in the next layer with associated weights. During the training process, these weights are adjusted to minimize the difference between the predicted output and the actual target values, typically using backpropagation and gradient descent.

x. Large Language Models:

Large Language Models are a class of machine learning models designed to understand and generate human language.

**Question b.**
"""

# Question b. load the dataset using pandas and split the data
data = pd.read_csv("indian_liver_patient.csv")
data.fillna(0,inplace=True)

variables =data.drop('Dataset', axis=1)
label = data['Dataset']

# Male:1, Female:0
variables['Gender'] = variables['Gender'].map({'Male': 1, 'Female': 0})

X_train, X_test, y_train, y_test = train_test_split(variables, label, test_size= 0.3, random_state=42)

"""**Question c.**"""

# Question c. Train and evaluate the below listed classifiers using the training and testing splits:
# i. Logistic regression classifier using sklearn.linear_model.LogisticRegression.
logreg = LogisticRegression(max_iter=10000)
lr_model=logreg.fit(X_train, y_train)

# train accuracy
y_LR = logreg.predict(X_train)
accuracy_score(y_train, y_LR)

# testing Accuracy
y_pred_LR = logreg.predict(X_test)
accuracy_score(y_test, y_pred_LR)

#ii. Support Vector Machine using sklearn.svm.
svm_classifier = SVC()
svc_model=svm_classifier.fit(X_train, y_train)

# train accuracy
y_svm = svm_classifier.predict(X_train)
accuracy_score(y_train, y_svm)

# test accuracy
y_pred_svm = svm_classifier.predict(X_test)
accuracy_score(y_test, y_pred_svm)

#iii. Decision tree using sklearn.tree.DecisionTreeClassifier.
dt_classifier = DecisionTreeClassifier()
dt_model = dt_classifier.fit(X_train, y_train)

# train accuracy
y_DT = dt_classifier.predict(X_train)
accuracy_score(y_train, y_DT)

# test accuracy
y_pred_DT = dt_classifier.predict(X_test)
accuracy_score(y_test, y_pred_DT)

#iv. LDA Classifier sklearn.discriminant_analysis.LinearDiscriminantAnalysis.
lda_classifier = LinearDiscriminantAnalysis()
lda_model=lda_classifier.fit(X_train, y_train)

# train accuracy
y_LDA = lda_classifier.predict(X_train)
accuracy_score(y_train, y_LDA)

# test accuracy
y_pred_LDA = lda_classifier.predict(X_test)
accuracy_score(y_test, y_pred_LDA)

"""**Question d. the training and testing accuracy of the four models:**


|  |Logistic regression classifier|  Support Vector Machine| Decision tree| LDA Classifier |
| --- | --- | --- | --- | --- |
|Training accuracy| 0.72 | 0.71 | 1.0 | 0.71|
|Testing accuracy | 0.73 | 0.73 | 0.68 | 0.73 |

Theory:

i.Logistic Regression Classifier:
Logistic regression models the probability that an instance belongs to a particular category. It uses the logistic function to squeeze the output of a linear equation between 0 and 1. The coefficients of the linear equation are estimated from the training data using the maximum likelihood method.

ii.SVM:
SVM aims to find the hyperplane that best divides a dataset into classes by maximizing the margin. The margin is defined as the distance between the hyperplane and the nearest data point from either set. For non-linearly separable data, SVM uses the kernel trick to transform data into a higher dimension where it is separable.

iii.Decision Tree:
A decision tree makes decisions based on asking a series of questions. For each attribute in the dataset, the decision tree algorithm forms a node, where the most important attribute is placed at the root node. It partitions the data into subsets that contain instances with similar values.

iv. LDA Classifier:
LDA focuses on finding the axis that best separates the data. It reduces dimensions, projects data onto a line, and classifies them. LDA tries to maximize the distance between means of two classes and minimize the variance within each class.

Logistic Regression, SVM, and LDA have very similar testing accuracies and training accuracies. This indicates these models are likely capturing similar patterns in the data and are generalizing similarly to the test set.
Decision Tree has a perfect training accuracy of 1.0, which indicates that it might be overfitting to the training data. This overfitting is evident in its lower testing accuracy compared to the other models.
From the presented results, Logistic Regression, SVM, and LDA seem to be the best choices for this dataset given their comparable and higher testing accuracies.

**Question e.**
"""

# Question e.
def select_knn_model(X_train, y_train, X_test, y_test):
    train_accuracies = []
    test_accuracies = []
    k_values = range(1, 21)

    # Iterate over all k values
    for k in k_values:
        # Create and fit the KNN classifier
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train)

        # Compute training accuracy
        train_pred = knn.predict(X_train)
        train_accuracy = accuracy_score(y_train, train_pred)
        train_accuracies.append(train_accuracy)

        # Compute testing accuracy
        test_pred = knn.predict(X_test)
        test_accuracy = accuracy_score(y_test, test_pred)
        test_accuracies.append(test_accuracy)

    # Plot training and testing accuracies
    plt.figure(figsize=(10, 6))
    plt.plot(k_values, train_accuracies, marker='o', label='Training Accuracy')
    plt.plot(k_values, test_accuracies, marker='s', label='Testing Accuracy')
    plt.xlabel('Value of k for KNN')
    plt.ylabel('Accuracy')
    plt.title('Training and Testing Accuracies vs. k')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Extract the optimal k value
    optimal_k = k_values[np.argmax(test_accuracies)]
    print(f"The optimal value of k is: {optimal_k}")

    return optimal_k

select_knn_model(X_train, y_train, X_test, y_test)

"""Based on the graph, when k is small, the model is overfit with high training accuracy and relative low testing accuracy. When k is greater than 6, the model has a reasonable(both high and similiar) training and testing accuracy, which means the model performs well on both training dataset and testing dataset. To prevent underfit when k gets larger, we pick 6 as the optimal value of k.

**Question f.**
Based on the graph below, the dataset is imbalanced because the number of patient has liver disease is significant larger than the patient does not have liver disease. This discrepancy in the distribution of instances between the two classes indicates an imbalance in the dataset.
When working with imbalanced datasets, Precision (Measures the number of correctly predicted positive observations out of the total predicted positive), Sensitivity (Measures the number of correctly predicted positive observations out of the all observations in actual class) and Confusion Matrix may be a good metric to evaluate the performance.
To balancec the dataset, one possible way is resampling.
"""

plt.hist(label)

"""**Question g.**"""

# Question g.
def get_top_features(model, X, y, n=3):
    # Compute permutation importance
    result = permutation_importance(model, X, y, n_repeats = 30,random_state=0)
    # Get the indices of the top n features
    sorted_idx = result.importances_mean.argsort()[-n:][::-1]
    # name of features
    top_features_names = variables.columns[sorted_idx]
    return top_features_names

# top 3 important features for Logistic regression classifier
get_top_features(lr_model, X_test, y_test)

# top 3 important features for SVM
get_top_features(svc_model, X_test, y_test)

# top 3 important features for Decision tree
get_top_features(dt_model, X_test, y_test)

# top 3 important features for LDA Classifier
get_top_features(lda_model, X_test, y_test)

"""Observations:

'Total_Protiens', 'Albumin_and_Globulin_Ratio', 'Albumin' are consistently ranked as the top 3 for Logistic Regression, SVM, and LDA. Their ranking order varies slightly.

The decision tree model has a different ranking, with only 'Albumin' being common with the above three models. 'Direct_Bilirubin' and 'Alkaline_Phosphotase' are specific to the decision tree model in this scenario.

Interpretation:

'Total_Protiens', 'Albumin_and_Globulin_Ratio', 'Albumin' appear to be universally important for three out of the four classifiers. However, the decision tree model stands out with a different ranking.
Different classifiers have distinct method for determining feature importance.
The commonality between the features selected by most classifiers indicates these features have strong predictive power across various models.
The decision tree model's divergence might indicate that it's catching some non-linear patterns or specific splits that the other models aren't emphasizing as much.
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# Label encoding for 'Gender' as it is categorical
label_encoder = LabelEncoder()
data['Gender'] = label_encoder.fit_transform(data['Gender'])

# Separate the features and the target
X = data.drop('Dataset', axis=1)
y = data['Dataset'] - 1  # Assuming 'Dataset' is the target and we want to start class labels from 0

# Standardize the features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Reduce the data to two principal components for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_std)

# A dummy model class as a placeholder for the actual model
# Replace this with your actual model and ensure it is trained before plotting
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Match the size of the first layer to the actual number of features, let's say 10
        self.fc1 = nn.Linear(10, 10)  # Adjust this to the actual number of features
        self.fc2 = nn.Linear(10, 5)
        self.fc3 = nn.Linear(5, 1)   # Output layer with one neuron for binary classification

    def forward(self, x):
        x = F.relu(self.fc1(x))   # ReLU activation for hidden layers
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))  # Sigmoid activation for output layer
        return x

# Instantiate and load your trained model here
# model = YourTrainedModelClass()
# model.load_state_dict(torch.load('path_to_your_trained_model.pth'))
model = Net()  # This is a placeholder for the actual model

# Generate a mesh grid to plot decision boundary
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                     np.arange(y_min, y_max, 0.01))

# Predict with the model on the mesh grid
grid = np.c_[xx.ravel(), yy.ravel()]
grid_tensor = torch.FloatTensor(grid)
with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    Z = model(grid_tensor).numpy()

# Reshape the prediction to match xx's dimensions
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Decision boundary and data points')
plt.show()